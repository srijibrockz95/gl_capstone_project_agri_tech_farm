To make the attached code work, please follow the below mentioned steps: 

	a. Make the static changes such as add your own account endpoint in the file. 
	b. Add the valid certificates and keys for the things you created in your AWS IoT Core. Place those things in config file and pass the relevant name in the driver code while creating the object. 
	c. You can change the topic name based on your requirement and implementation technique. 
	d. After making these above changes you should also import relevant libraries to make the code work. If a library is not installed please install it and then run the code.
	e. To execute the code you can use terminal, navigate to the directiry where file is stored and path to the keys are provided, type <python filename.py>. If all the library and files are avilable and correct you shoul see the output on console. 
	f. Move to AWS Iot core subscribe to the topic and check the output coming there. 

Here is an official AWS link we used to implement the above code: https://aws.amazon.com/premiumsupport/knowledge-center/iot-core-publish-mqtt-messages-python/


-----------------------------------------------------------------------------------------------------------------------
Setting up Kinesis Data Stream,Kinesis Delivery Stream and Kinesis Analytics App:
-----------------------------------------------------------------------------------------------------------------------
1.Create following IAM Roles before starting with Kinesis Flow:
    1.IOT_TO_Kinesis
    Permissions policies: AWSIoTThingsRegistration,AWSIoTLogging,AWSIoTRuleActions,AmazonKinesisFullAccess
    2.FirhoseToS3
    Permissions policies: AmazonS3FullAccess,AmazonKinesisFullAccess
    3.kinesis-analytics-App01-us-east-1
    Permissions policies: AmazonKinesisFullAccess,AWSLambda_FullAccess

2.Create Kinesis Data Stream:
Kinesis->Data Stream-> Create Data Stream:
    Name:raw_data_stream
    Data Stream Capacity:
        Capacity mode:Provisioned
        Provisioned shards:1

3.Create  IOT Rule to transfer Data from IOT Core to Kinesis Data Stream:
AWS IOT Core->Act->Rules->Create Rule:
    Rule Name:IOTTTOKinesis
    Configure SQL Statement:
        Specify SQL Version:2016-03-23
        SQL Statement:SELECT * FROM 'iot/agritech'
        Rule Actions:Kinesis Stream
            Stream Name:raw_data_stream
            Partition key:sprinkler_id
            IAM Role:IOT_TO_Kinesis
        Create.

4.Create Delivery Stream(Data Firehose):
Kinesis->Delivery Streams->Create Delivery Stream:
    Source: Amazon Kinesis Data Streams
    Destination: Amazon S3
    Source Settings:
        Kinesis Data Stream: raw_data_stream
    Delivery stream name: raw_delivery_stream
    Destination Settings:
        S3 bucket:capstoneagritechfarm20
    Advanced Settings:
        Permissions:
            Choose existing IAM role:FirehoseToS3

5. Create Kinesis SQL Legacy App:
Kinesis->Analytics Applications->SQL applications (legacy)->Create SQL Application(Legacy)->
    Application Name: App-01
    Create

    1.Source->Configure:
    Source:Kinesis Data Stream
    Kinesis Data Stream: raw_data_stream
    IAM role for reading source stream: kinesis-analytics-App01-us-east-1
    Discover Schema: Check for the values populating and Save Changes
    Save Changes

    2.Real-time analytics->Configure:
        -Add the SQL Code(Refer to gl_capstone_project_agri_tech_farm/Kinesis/Kinesis_Analytics_SQL_Code for sql code) in code editor.
        -Save and run application.
        -After running,2 inline streams gets created:AGGREGATE_SQL_STREAM,ANOMALY_SQL_STREAM
        -Connect streams to destination:
        (Note:For Connecting to destination, first create the Lambda functions defined in next step. At last connect these destinations)
            Connect Destination for AGGREGATE_SQL_STREAM:Aggregatefn
            Connect Destination for ANOMALY_SQL_STREAM:Anomalyfn

------------------------------------------------------------------------------------------------------------------
Setting up Dynamodb, sns, Lambdas and python flask dashboard
------------------------------------------------------------------------------------------------------------------
Dynamodb and sns:
1.	Update Endpoint in dbsetup_utils.py file inside the "create_sns" method with your email address
2.	Run the file dbsetup_main.py file to create DynamoDB tables and sns
3.	Open your email inbox and confirm sns subscription.

Lambdas:
Go to AWS lambda console.

Create a layer in Lambda:
1.	Create a layer in Lambda by clicking on the Layers menu on the left side of AWS Lambda console
2.	Give name as “pyowm”.
3.	Upload python.zip file which can be found inside Lambda folder of solution.
4.	Select python 3.9 as runtime and x86_64 as architecture.
5.	Click on Create button.

Create Lambdas:

1.	Create a lambda function named Aggregatefn and select “Create a new role with basic lambda permissions”, Runtime – Python 3.9, Architecture – x86_64
2.	Copy the contents of Aggregate_Lambda_Handler.py file under Lambda folder of solution to the lambda code. 
3.	Change the lambda timeout to 3 mins under configuration tab.
4.	Attach policies for accessing Cloud watch, DynamoDB, IoT and SNS to the newly created role.
5.	Create another lambda function named Anomalyfn and copy the contents of Anomaly_Lambda_Handler.py file to the lambda code.
6.	Change the lambda timeout to 3 mins under configuration tab.
7.	Add a layer for Anomalyfn lambda and add pyown layer as custom layer
8.	Use the same role that was created for the Aggregatefn.

Python Flask Dashboard:
	Run the file display.py under the webapp folder and go to: http://127.0.0.1:5000/ sensor and sprinkler dashboard can be seen.



